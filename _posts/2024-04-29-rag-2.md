---
layout:     post
title:      一文打尽Topic Model方法——RAG系列(2)
subtitle:   Topic Model方法各种流派的方法记录
date:       2024-04-29
author:     Wade
header-img: img/WechatIMG97.jpg
catalog: true
tags:
    - RAG
    - 增强检索
    - AIGC
---
# 前言

隔了很久终于写这第二篇文章了，这个系列针对的是 RAG 实战中，工程师实际要应对的工程卡点，技术卡点，欢迎点赞关注一波。上篇文章讲了下 pdf 的解法，这片文章我们关注一下文本的 topic 抽取。

在 RAG（Retrieval-Augmented Generation）的实战中，工程师们经常会遇到各种工程和技术难题，而主题抽取正是这些难题中的基础。无论是 Query 的 topic 抽取、情绪识别、document 的自动分类，还是元数据构造，这些都属于 RAG 的基础设施。这块就是打地基，可以说，许多个性化功能的实现都依赖于这些基础的分类和识别技术。

那现在市面上有什么技术流派来解决这个问题呢？

# T0: LLM 原生

• 优点：效果好，精准，基本是是 SOTA 级别（GPT4-turbo）

• 缺点：上下文长度限制，响应时间慢，费用高，调优难，多任务难以并行

这就是为什么，当前情况，T0 效果这么好的情况下，还要通过其他路径，去趋近 LLM 的效果的原因

# T1: Embedding 向量空间的相似度聚合

## cohere 实践

[https://docs.cohere.com/page/topic-modeling](https://docs.cohere.com/page/topic-modeling)
主题建模是一种用于从大量文本文档中提取主要主题的技术。它可以帮助人们理解大量非结构化文本数据，例如聊天机器人收到的消息。例如，这在客户服务方面非常有用，团队可以分析客户对产品或服务提出的常见问题和反馈，从而在未来更好地为客户服务。

Topic Modeler 示例应用分析了由人们向人工智能个人助理发出的指令组成的数据集，然后提取了数据集的关键主题和主题。它利用文本嵌入（文本数据的数字表示，可捕捉其含义），并使用 Cohere Embed 端点检索这些文本嵌入。主题是通过聚类算法生成的，它根据不同文本输入所代表的主题将其分为不同的群组。

建立主题建模器的步骤如下  
第 1 步：加载文本数据集

第 2 步：创建聚类

第 3 步：获取聚类关键词

第 4 步：在图表上可视化聚类

核心代码如下：

```
api_key = “PASTE_YOUR_API_KEY_HERE”
co = cohere.Client(api_key)

 Embed with Cohere’s embedding model, then convert into a numpy array
embeddings = co.embed(texts=list(df[‘utt’]),
                       truncate="RIGHT").embeddings
embeddings = np.array(embeds)

cluster_model = KMeans(n_clusters=n_clusters)
topic_model = BERTopic(hdbscan_model=cluster_model)

# df is a dataframe. df['title'] is the column of text we're modeling
df['topic'], probabilities = topic_model.fit_transform(df['utt'], embeddings)
```

这里面用了 BertTopic，也放一点内容看看

## BertTopic

《BERTopic: Neural topic modeling with a class-based TF-IDF procedure》
为了克服 Top2Vec 的缺点，BertTopic 并不是把文档和词都嵌入到同一个空间，而是单独对文档进行 embedding 编码，然后同样过降维和聚类，得到不同的主题。但在寻找主题表示时，是把同一个主题下的所有文档看成一个大文档，然后通过 c-TF-IDF 最高的 NNN 个词作为该主题表示。简单点说，BerTopic 寻找主题表示时用的是 bags-of-words。

算法流程

• 把文档映射到 embedding 空间

• 这里可以用预训练模型，像 BERT、RoBERTa，或者经过 fine tuning 的预训练模型进行文档编码，相对于 Top2Vec 用的 Doc2Vec，预训练模型无疑表征能力更强

• 降维。同 Top2Vec，用的 UMAP

• 聚类。同 Top2Vec，用的 HDBSCAN

• 主题表示。如上面所提，把属于同一个 topic 的所有文档看成一个大文档。然后算 c-TF-IDF，公式如下，变量含义可以看论文

这个做法简单明了，又有效，很棒。

## Top2vec

比如[https://github.com/ddangelov/Top2Vec](https://github.com/ddangelov/Top2Vec)
[https://arxiv.org/pdf/2008.09470](https://arxiv.org/pdf/2008.09470)

下面 top2vec 是来自周星星的知乎文章[https://www.zhihu.com/question/34801598/answer/2776359659](https://www.zhihu.com/question/34801598/answer/2776359659)

原理：用 Doc2vec 技术，把文档和词都映射到同一个语义向量空间。对文档向量进行聚类，分成不同的簇，每个簇就代表一个主题，同一个簇的文档向量求 average，得到的向量作为该主题的 Topic 表示，再用离该 topic vector 最近的 NNN 个词作为该 Topic 的表示。

算法流程

• 使用 Doc2Vec 创建文档和字的联合向量空间（也称为 embedding 嵌入空间）

• 这里的 Doc2Vec 类似 word2vec 中的 skip-gram，用的是比较浅的网络

![](https://files.mdnice.com/user/57315/b3929318-01bd-4828-a286-174985ca74ca.png)

• 对文档向量使用 UMAP 进行降维

• 降维的目的是避免维度灾难，提高后续聚类算法的效果和效率

• UMAP 18 年提出，目前是当红炸子鸡的降维算法，属于非线性流形降维。由于 embedding 空间通常有着**各向异性**的问题，用 PCA 这种线性降维效果是不适合的，而 UMAP 相对于 t-SNE，能更好保留原空间的局部和全局结构，且计算效率更好

• 使用 HDBSCAN 进行聚类

• 密度聚类优势能天然挖掘异常点。从业务上看，有一些文档就是不属于某一个主题的，强行划分会影响效果

• 同样是因为 embedding 空间的各向异性，并不适合用 K-means 这种 centroid-based 的聚类方案

• HDBSCAN 相比于 DBSCAN，不用定义领域半径 eps 和密度阈值 MinPt 这两个刺手的参数

• 关于 HDBSCAN 的深入理解，可以看这篇 How HDBSCAN Works

• 聚类得到的每个簇为一个主题，对同一个簇的所有文档向量求平均，得到该主题的向量

![](https://files.mdnice.com/user/57315/e6d1742c-3b61-4eb3-8f52-ec0592ef752c.png)

• 每个主题离该主题向量最近的 NNN 个词向量对应的词来作为主题表示

• 不需要像 LDA 一样做停用词，常用词停用词通常分布在不同的主题向量中间，不会跟某一个具体的主题很近

![](https://files.mdnice.com/user/57315/a150c99c-b677-4896-a7ae-dde6bd52af2c.png)

优点

• 通过 HDBSCAN 自动发现主题数

• 不需要停用词表

• 不需要 stemming/lemmatization，因为这里是用 embedding 来表示一个文档，而不是用 bags-of-words

• 相比于 bags-of-words，用 embedding 无疑对文本的表征能力更强

• 把词、文档、主题都用同一个 embedding 空间表示，能实现一些实用的功能。如搜索一个词的相似词、一个文档的相似文档、输入主题搜索最相关的文档、输入词搜于文档等，由于有向量，这些搜索都能相当丝滑

缺点

• 一个很有意思的点是。聚类选择的是 HDBSCAN 这种 density-based 的聚类方法，但是在求主题向量时，是从 centroid-based 的角度，即是通过同一簇下的向量求平均得到主题向量，这会导致得到的主题向量是不准确的，从而造成主题表示是不准确的。

实际上，作者在论文里，有实验过几种方法，但 average embedding 是最简单和有效的。

## T1: LDA 机器学习方法永不为奴（bushi）

机器学习相关的，主要看 1 是否有监督，2 词袋统计还是 embedding 相似度

1 非监督，词袋概率最经典的，LDA，这是一种经典的非监督机器学习方法。LDA 基于词袋模型和概率统计，是主题建模领域的老牌强者。

2 集合词嵌入合 LDA 的项目：[https://github.com/MilaNLProc/contextualized-topic-models](https://github.com/MilaNLProc/contextualized-topic-models)

3 半监督，词袋概率：[https://github.com/gregversteeg/corex_topic](https://github.com/gregversteeg/corex_topic)

类似的还可以参考下知乎这篇文章：[https://www.zhihu.com/question/34801598/answer/2776359659](https://www.zhihu.com/question/34801598/answer/2776359659)

如果你对这些主题模型的更多细节感兴趣，可以查看以下资源。比如，cohere 的官方文档、BertTopic 的相关论文，以及 Top2Vec 的 GitHub 页面和论文。
好了，今天的分享就到这里。希望这篇文章能够帮助你更好地理解主题模型的世界。如果你有任何问题或建议，请随时留言。别忘了点赞和关注哦，我们下期再见！
